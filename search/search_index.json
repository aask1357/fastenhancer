{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FastEnhancer","text":"<p>github | paper.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Installation<ul> <li>Installation for all features</li> <li>Minimal installation for training</li> <li>Minimal installation for calculating objective metrics</li> <li>Minimal installation for ONNX exporting</li> <li>Minimal installation for ONNXRuntime (spec2spec version)</li> <li>Minimal installation for ONNXRuntime (wav2wav version)</li> </ul> </li> <li>Training<ul> <li>Training code</li> <li>Training recipes<ul> <li>FastEnhancer</li> <li>BSRNN</li> <li>FSPEN</li> <li>LiSenNet</li> </ul> </li> <li>Experience sharing</li> </ul> </li> <li>Calculating objevtice metrics</li> <li>ONNXRuntime<ul> <li>Exporting to ONNX and executing ONNXRuntime</li> <li>Executing ONNXRuntime</li> </ul> </li> </ul>"},{"location":"metrics/","title":"Objective Metrics","text":"<p>We support - DNSMOS P.808 and P.835 (SIG, BAK, OVL) - SCOREQ (github)     - It provides two domains: Natural Speech domain trained for real recordings, and Synthetic Speech domain trained for TTS.     - We use Natural Speech domain.     - It provides two modes: no-reference (NR) and non-matching reference (NMR). NR mode is a non-intrusive mode. NMR mode takes reference and degraded signals. However, the reference doesn't have to be the clean speech pair of the degraded signal.     - We use full reference mode, meaning clean speech is given as the reference of the NMR mode. - SISDR     - On some implementations, mean values of reference and degraded signal are substracted. However, we don't perform mean substraction. - PESQ (github)     - We use P.862.2 without Corrigendum 2. For more information, see the following paper and github. - STOI and ESTOI (github) - WER     - Whisper-large-v3-turbo is used.     - For Voicebank-Demand testset, there exists transcript, so we can calculate WER.     - For DNS-Challenge dev testset (Interspeech2020, synthetic), transcript is not provided. Furthermore, each clean speech is a random crop of a long audio, so the beginning and end of the speech are cut off. Therefore, it is impossible to create a transcript. We don't calculate WER for DNS-Challenge.  </p>"},{"location":"metrics/#calculating-objective-metrics","title":"Calculating objective metrics","text":"<p>Assume you want to calculate objective metrics for a model in <code>logs/fastenhancer_l</code>. For Voicebank-Demand, run the following code:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python -m scripts.metrics_ns -n fastenhancer_l --transcript-dir PATH-TO-TRANSCRIPT</code></pre> <p>For DNS-Challenge, run the following code:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python -m scripts.metrics_ns -n fastenhancer_l --wer False</code></pre> <p>If you want to use cuda:1, don't set <code>-d cuda:1</code>. Set <code>CUDA_VISIBLE_DEVICES=1</code> and <code>-d cuda:0</code>.</p>"},{"location":"onnx/","title":"ONNX","text":"<p>You can export your own model to ONNX and execute ONNXRuntime. You can also download pre-compiled ONNX file and execute ONNXRuntime.  </p> <p>There are two ways for ONNXRuntime.  </p> <ol> <li>Spectrogram-to-spectrogram (spec2spec) version: STFT and iSTFT are done in PyTorch. Only the neural network part is calculated in ONNXRuntime. This version is exported using torch.export.  </li> <li>Waveform-to-waveform (wav2wav) version: STFT and iSTFT are also done in ONNXRuntime. This version is exported using torchdynamo. Only FastEnhancers are successfully exported to this version. For other models, it is either impossible to create a wav2wav version or the ONNXRuntime execution speed is very slow.  </li> </ol> <p>The RTFs in our paper are measured using spec2spec versions.</p>"},{"location":"onnx/#exporting-to-onnx-and-executing-onnxruntime","title":"Exporting to ONNX and executing ONNXRuntime","text":"<p>Suppose you have trained a model and saved its checkpoints at <code>logs/fastenhancer_l</code>. The code below exports the model to ONNX, saves to <code>onnx/fastenhancer_l.onnx</code>, executes the ONNXRuntime, and calculates the RTF. Wav2wav version:</p> <pre><code>python -m scripts.export_onnx -n fastenhancer_l --onnx-path onnx/fastenhancer_l.onnx</code></pre> <p>Spec2spec version:</p> <pre><code>python -m scripts.export_onnx_spec -n fastenhancer_l --onnx-path onnx/fastenhancer_l.onnx</code></pre>"},{"location":"onnx/#executing-onnxruntime","title":"Executing ONNXRuntime","text":"<p>You can download a pre-compiled ONNX file from here. If you downloaded a wav2wav version in <code>onnx/fastenhancer_t.onnx</code>, run the following code:</p> <pre><code>python -m scripts.test_onnx --onnx-path onnx/fastenhancer_t.onnx</code></pre> <p>If you downloaded a spec2spec version in <code>onnx/fastenhancer_t_spec.onnx</code>, run the following code:</p> <pre><code>python -m scripts.test_onnx_spec --onnx-path onnx/fastenhancer_t_spec.onnx</code></pre> <p>There are some model-specific settings. - For FastEnhancer-M, you should set <code>--hop-size 160</code>:  <pre><code>python -m scripts.test_onnx --onnx-path onnx/fastenhancer_m.onnx --hop-size 160</code></pre></p> <ul> <li> <p>For FastEnhancer-L, you should set <code>--hop-size 100</code>:  <pre><code>python -m scripts.test_onnx --onnx-path onnx/fastenhancer_l.onnx --hop-size 100</code></pre></p> </li> <li> <p>For GTCRN, you should set <code>--win-type hann-sqrt</code>:  <pre><code>python -m scripts.test_onnx_spec --onnx-path onnx/gtcrn_spec.onnx --win-type hann-sqrt</code></pre></p> </li> <li> <p>For other models, use default settings.</p> </li> </ul>"},{"location":"onnx/#more-information-about-wav2wav-version","title":"More information about wav2wav version","text":"<p>Let <code>x</code> denote a noisy input and <code>y</code> denote an enhanced signal. Let <code>n</code> denote an fft size and <code>h</code> denote a hop size. In the wav2wav version, at every <code>i</code>-th iteration, the model gets <code>x[i*h+(n-h):i*h+n]</code> as an input and returns <code>y[i*h:(i+1)*h]</code>. This means that the input and the output has a delay of <code>n-h</code>.  </p> <p>Why?  </p> <p>Obviously, At the first iteration, the model takes <code>x[0:n]</code> and generates an enhanced signal <code>y[0:n]</code>. At the second iteration, the model takes <code>x[h:n+h]</code> and generates an enhanced signal <code>y[h:n+h]</code> which is overlap-added to the previous iteration's output.  </p> <p>However, This implies that at the end of first iteration, <code>y[0:h]</code> is completed while <code>y[h:n]</code> isn't. Also, at the beggining of second iteration, only <code>x[n:n+h]</code> is a new input. <code>x[h:n]</code> was already given at the first iteration.  </p> <p>So, At the first iteration, the model saves <code>x[h:n]</code> as its input cache and <code>y[h:n]</code> as its output cache. At the second iteration, the model gets <code>x[n:n+h]</code> as an input. The model concatenate its input cache with the new input to make <code>x[h:n+h]</code>. The model generates <code>y[h:n+h]</code>. It is overlap-added with the previous output cache <code>y[h:n]</code>. Since <code>y[h:2*h]</code> is now completed, it is returned. The model caches <code>x[2*h:n+h]</code> and <code>y[2*h:n+h]</code> for the next iteration.  </p> <p>The final algorithm is as below:  </p> <ul> <li>Initially, the model has an input cache <code>cache_in</code> whose length is <code>n-h</code> and filled with zeros. The model also has an output cache <code>cache_out</code> whose length is <code>n-h</code> and filled with zeros.  </li> <li>At every iterations, the model gets a new input chunk <code>x</code> with a length of <code>h</code>.  </li> <li> <p>The model concatenate the input chunk with its input cache to create an input with a length of <code>n</code>:  <pre><code>x = torch.cat([cache_in, x])</code></pre></p> </li> <li> <p>The model saves the last <code>n-h</code> samples as its new input cache:  <pre><code>cache_in = x[h:n]</code></pre></p> </li> <li> <p>The model generates an enhanced signal <code>y</code> with a length of <code>n</code>:  <pre><code>y = model(x)</code></pre></p> </li> <li> <p>The model performs an overlap-add:  <pre><code>y[0:n-h] += cache_out</code></pre></p> </li> <li> <p>The model saves the last <code>n-h</code> samples as its new output cache:  <pre><code>cache_out = y[h:n]</code></pre></p> </li> <li> <p>The model returns the first <code>h</code> samples:  <pre><code>return y[0:h]</code></pre></p> </li> </ul> <p>Outside the model, it seems that the model gets an input with a length of <code>h</code> and returns an output with a length of <code>h</code>. However, you now understand that those input and output are not time-aligned. They have a time difference of <code>n-h</code>.</p>"},{"location":"dataset/","title":"Dataset","text":"<p>This section describes about dataset preparation. For training, we need four types of dataset for: - Training - Validation - Inferencing (During training, every <code>infer.interval</code> epochs, we perform inference for a small amount of data and write the tensorboard log). - Calculating objective metrics (We re-use valid set).</p>"},{"location":"dataset/#supported-datasets","title":"Supported Datasets","text":"<ul> <li>Voicebank-Demand</li> <li>DNSChallenge</li> </ul>"},{"location":"dataset/dns-challenge/","title":"DNS-Challenge Datasets","text":"<p>DNS-Challenge datasets are noise suppression dataset.  </p> <ul> <li>DNS-Challenge 1 (Interspeech 2020): Widebank(16kHz), 500 hours of clean speech from LibriVox audiobooks. It contains a synthetic dev test set with <code>clean</code> and <code>noisy</code> pairs that can be used for calculating intrusive objective metrics. It also contains a real recording dev test set and a blind test set that can be used for calculating non-intrusive objective metrics and subjective metrics.  </li> <li>DNS-Challenge 2 (ICASSP 2021): Wideband(16kHz), 760 hours of clean speech (singing voice, emotion data, and non-English are added).  </li> <li>DNS-Challenge 3 (Interspeech 2021): Wideband(16kHz) &amp; Fullband(48kHz).  </li> <li>DNS-Challenge 4 (ICASSP 2022): Fullband(48kHz). Personalized train dataset added.  </li> <li>DNS-Challenge 5 (ICASSP 2023): Fullband(48kHz). Headset dataset added.  </li> </ul> <p>For 16kHz dataset, we recommend to download DNS-Challenge 3 wideband train dataset and DNS-Challenge 1 synthetic testset. For 48kHz dataset, we recommend to download DNS-Challenge 5 speakerphone train dataset. Note that no test set with <code>clean</code> and <code>noisy</code> pairs is provided. Test sets with only <code>noisy</code> files are provided.  </p>"},{"location":"dataset/dns-challenge/#preparing-dataset","title":"Preparing dataset","text":""},{"location":"dataset/dns-challenge/#download","title":"Download","text":"<p>Download the dataset from here. For DNS-Challenge 1 synthetic dev test set, we provide a pre-processed version here</p>"},{"location":"dataset/dns-challenge/#downsample","title":"Downsample","text":"<p>If needed, downsample the dataset using <code>scripts/resample.py</code>. For example, if you want to downsample to 24kHz, run the code below:</p> <pre><code>python -m scripts.resample --to-sr 24000 --from-dir ~/Datasets/DNS_Challenge/dataset_fullband --to-dir ~/Datasets/DNS_Challenge/dataset_24khz</code></pre>"},{"location":"dataset/dns-challenge/#modify-configuration-file","title":"Modify Configuration file","text":"<p>You have to change the dataset path and sampling rate of configuration file. Since there's no official testset composed of <code>clean</code> and <code>noisy</code> pairs for fullband, you can train with DNS-Challenge dataset and validate with other dataset. For example, in <code>configs/fastenhancer/huge_dns.yaml</code>, we trained <code>FastEnhancer-Huge-noncausal</code> at 24kHz using DNS-Challenge dataset and validated using Voicebank-demand testset. Modify various configurations in <code>data</code> section of the config file as you wish. If the dataset loading is too slow, you may consider increasing <code>train.num_workers</code>. If the speed is still slow, we recommend to write a code to synthesize dataset in advance and train using the synthesized dataset.  </p>"},{"location":"dataset/dns-challenge/#dataset-code","title":"Dataset Code","text":"<p>For DNS-Challenge dataset, we load clean speech, noise, and optionally RIR. They are later mixed on-the-fly at the training code. To check or modify the dataset code, see <code>utils/data/ns_on_the_fly.py</code>.</p>"},{"location":"dataset/dns-challenge/#training-code","title":"Training Code","text":"<p>To check or modify the training code for DNS-Challenge, see <code>wrappers/ns_on_the_fly.py</code>. Clean speech and noise pairs are loaded and mixed on-the-fly to generate noisy.  </p> <p>If you want to add RIR to clean speech, you have to modify the code.  </p> <ul> <li><code>config.yaml</code> </li> <li>Set <code>data.reverb_prob</code> higher than 0 and leq than 1.  </li> <li>Set <code>data.rir_length</code> to the max length of rir. For DNS-Challenge synthetic RIR, set it to 2 seconds (32000 for 16kHz and 96000 for 48khz).  </li> <li><code>wrappers/ns_on_the_fly.py</code> </li> <li>Add <code>rir</code> to <code>self.keys</code> </li> <li>Load <code>batch['rir']</code>, and give it to <code>self.snr_mixer</code>.  </li> </ul> <p>In this case, the rir-convolved clean speech becomes the target, which means your model doen't perform dereverberation. If you want to do dereverberation along with noise suppression, you have to implement on your own. Some papers preserve only the first 100ms reflections and use that as a target (GTCRN, UL-UNAS). In URGENT challenges, they find the rir_start_index and preserve the 50ms reflections from that starting point.</p>"},{"location":"dataset/voicebank-demand/","title":"Voicebank-Demand Dataset","text":"<p>Voicebank-Demand, also known as VCTK-Demand, is a noise suppression dataset with a sampling rate of 48kHz. There are two train datasets: one is a 28-speaker version, and the other is a 56-speaker version. In many papers, including ours, the 28-speaker version is used.  </p>"},{"location":"dataset/voicebank-demand/#preparing-dataset","title":"Preparing dataset","text":""},{"location":"dataset/voicebank-demand/#download","title":"Download","text":"<p>Download the train data, test data, and logfiles from here. Download a trainscript file of the testset from here.</p>"},{"location":"dataset/voicebank-demand/#downsample","title":"Downsample","text":"<p>If needed, downsample the dataset using <code>scripts/resample.py</code>. For example, if you want to downsample to 16kHz, run the code below:</p> <pre><code>python -m scripts.resample --to-sr 16000 --from-dir ~/Datasets/voicebank-demand/48k --to-dir ~/Datasets/voicebank-demand/16k</code></pre> <p>After downloading, the directory may look like this:</p> <pre><code>voicebank-demand\n\u251c\u2500 16k\n|  \u251c\u2500 clean_testset_wav\n|  \u251c\u2500 clean_trainset_28spk_wav\n|  \u251c\u2500 noisy_testset_wav\n|  \u2514\u2500 noisy_trainset_28spk_wav\n\u251c\u2500 48k\n|  \u251c\u2500 clean_testset_wav\n|  \u251c\u2500 clean_trainset_28spk_wav\n|  \u251c\u2500 noisy_testset_wav\n|  \u2514\u2500 noisy_trainset_28spk_wav\n\u2514\u2500 logfiles\n   \u251c\u2500 log_readme.txt\n   \u251c\u2500 log_testset.txt\n   \u251c\u2500 log_trainset_28spk.txt\n   \u2514\u2500 transcript_testset.txt</code></pre>"},{"location":"dataset/voicebank-demand/#modify-configuration-file","title":"Modify Configuration file","text":"<p>You have to change the dataset path and sampling rate of configuration file. For example, to train FastEnhancer-B, change <code>data</code> section in <code>configs/fastenhancer/b.yaml</code>.</p>"},{"location":"dataset/voicebank-demand/#dataset-code","title":"Dataset Code","text":"<p>For Voicebank-demand dataset, we load clean and noisy speech pairs. To check or modify the dataset code, see <code>utils/data/voicebank_demand.py</code>.</p>"},{"location":"dataset/voicebank-demand/#training-code","title":"Training Code","text":"<p>To check or modify the training code for Voicebank-Demand, see <code>wrappers/ns.py</code>. Clean and noisy pairs are loaded for training.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-for-all-features","title":"Installation for all features","text":"<p>First, follow Installation for Training. Second, install the following packages:</p> <pre><code>pip install torchmetrics jiwer onnx onnxsim onnxscript\npip install git+https://github.com/openai/whisper.git</code></pre> <p>Third, install onnxruntime-gpu. Make sure to install the version that matches your CUDA version. If you cannot install the GPU version, you can install the CPU version instead. However, DNSMOS and SCOREQ will run on CPU and the <code>metrics_ns.py</code> code will run very slow.</p>"},{"location":"installation/#minimal-installation-for-training","title":"Minimal installation for training","text":"<p>Required by <code>train.py</code> and <code>train_torchrun.py</code>.  </p> <p>Refer to Installation for Training.</p>"},{"location":"installation/#minimal-installation-for-calculating-objective-metrics","title":"Minimal installation for calculating objective metrics","text":"<p>Required by <code>metrics_ns.py</code>.  </p> <p>First, follow Installation for Training. Second, install the following pacakges:</p> <pre><code>pip install torchmetrics jiwer\npip install git+https://github.com/openai/whisper.git</code></pre> <p>Finally, install onnxruntime-gpu. Make sure to install the version that matches your CUDA version. If you cannot install the GPU version, you can install the CPU version instead. However, DNSMOS and SCOREQ will run on CPU and the <code>metrics_ns.py</code> code will run very slow.</p>"},{"location":"installation/#minimal-installation-for-onnx-exporting","title":"Minimal installation for ONNX exporting","text":"<p>Required by <code>scripts/export_onnx.py</code> and <code>scripts/export_onnx_spec.py</code>.  </p> <p>First, follow Installation for Training. Second, install the following pacakges:</p> <pre><code>pip install onnx onnxsim onnxscript</code></pre>"},{"location":"installation/#minimal-installation-for-onnxruntime-spec2spec-version","title":"Minimal Installation for ONNXRuntime (spec2spec version)","text":"<p>Required by <code>scripts/test_onnx_spec.py</code>.  </p> <p>First, install PyTorch. It doesn't need to be GPU version. Second, install the following pacakges:</p> <pre><code>pip install numpy scipy librosa tqdm</code></pre> <p>Finally, install onnxruntime. It doesn't matter whether you intsall a CPU version or a GPU version. Even if you install a GPU version, the code will run on CPU anyway.</p>"},{"location":"installation/#minimal-installation-for-onnxruntime-wav2wav-version","title":"Minimal Installation for ONNXRuntime (wav2wav version)","text":"<p>Required by <code>scripts/test_onnx.py</code>.  </p> <p>You don't need to install PyTorch in this case. First, install the following pacakges:</p> <pre><code>pip install numpy scipy librosa tqdm</code></pre> <p>Then install onnxruntime. It doesn't matter whether you intsall a CPU version or a GPU version. Even if you install a GPU version, the code will run on CPU anyway.</p>"},{"location":"installation/training/","title":"Installation for Training","text":"<p>We tested under: - PyTorch==2.7.1, CudaToolkit==11.8, Python==3.13 - PyTorch==2.7.1, CudaToolkit==12.8, Python==3.13  </p> <p>Note that we failed under PyTorch==2.8.0. - We succeeded to train, calculate metrics, export ONNX spec2spec, and execute ONNXRuntime. - However, we failed to export ONNX wav2wav. After downgrading to PyTorch==2.7.1, the problem is solved.  </p>"},{"location":"installation/training/#0-decide-python-cuda-toolkit-and-pytorch-versions","title":"(0) Decide Python, CUDA toolkit, and PyTorch versions","text":"<p>Before install, you have to decide which version to install (including Python, CUDA toolkit, and PyTorch). Note that <code>PyTorch&gt;=2.3</code> is recommended. On <code>PyTorch&lt;2.3</code>, <code>torch.nn.utils.parametrizations.weight_norm</code> is not implemented, so you have to change the codes and .yaml files. You also have to remove <code>device_id</code> argument of <code>dist.init_process_group</code> in <code>train.py</code>.  </p> <p>First, check CUDA toolkit versions that your nvidia driver supports:</p> <pre><code>nvidia-smi | grep \"CUDA Version\"\n</code></pre> <p>The output should look like this:</p> <pre><code>| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |\n</code></pre> <p>That is the maximum CUDA toolkit version you can install. In our case, we can choose any version <code>&lt;= 13.0</code>.  </p> <p>Second, visit here and decide PyTorch, Python, and CUDA toolkit version. Then install Python to your environment. For the rest of this document, we will use <code>torch-2.7.1+cu128-cp313</code> version, meaning PyTorch <code>2.7.1</code>, CUDA toolkit <code>12.8</code>, and Python <code>3.13</code>. You can use your favorite environment manager. We use miniconda as below:</p> <pre><code>conda create -n fastenhancer python=3.13 -c conda-forge\nconda activate fastenhancer\n<p></p>"},{"location":"installation/training/#1-install-cuda-toolkit-and-cudnn","title":"(1) Install CUDA toolkit and cuDNN","text":"<p>Download a local runfile of CUDA toolkit and install.\nIn the following example, we will install CUDA toolkit <code>12.8</code> in <code>/home/shahn/.local/cuda-12.8</code>:</p>\n<pre><code>wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda_12.8.1_570.124.06_linux.run\n\nchmod +x cuda_12.8.1_570.124.06_linux.run\n\n./cuda_12.8.1_570.124.06_linux.run \\\n  --silent \\\n  --toolkit \\\n  --installpath=/home/shahn/.local/cuda-12.8 \\\n  --no-opengl-libs \\\n  --no-drm \\\n  --no-man-page</code></pre>\n<p>\nThen, install a tar file of cuDNN for your CUDA version. In the following example, we download cuDNN <code>8.9.7</code> for CUDA <code>12.x</code> and install as below:</p>\n<pre><code>tar xvf cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz --strip-components=1 -C /home/shahn/.local/cuda-12.8</code></pre>\n\n<p>Finally, set environment variables</p>\n<pre><code>export CUDA_HOME=/home/shahn/.local/cuda-12.8\nexport PATH=$CUDA_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$CUDA_HOME/lib:$LD_LIBRARY_PATH</code></pre>\n<p>and check:</p>\n<pre><code>which nvcc\n\nnvcc --version\n</code></pre>\n<p>Then the output should look like this:</p>\n<pre><code>/home/shahn/.local/cuda-12.8/bin/nvcc\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2025 NVIDIA Corporation\nBuilt on Fri_Feb_21_20:23:50_PST_2025\nCuda compilation tools, release 12.8, V12.8.93\nBuild cuda_12.8.r12.8/compiler.35583870_0\n</code></pre>"},{"location":"installation/training/#2-install-pytorch-and-torchaudio","title":"(2) Install PyTorch and Torchaudio","text":"<p>Check which torchaudio version matches your PyTorch version at here.\nThen install approriate version of PyTorch and Torchaudio.\nIn the following example, we install PyTorch <code>2.7.1</code>, CUDA <code>12.8</code> as below:</p>\n<pre><code>pip install torch==2.7.1+cu128 torchaudio==2.7.1+cu128 --index-url https://download.pytorch.org/whl\n</code></pre>"},{"location":"installation/training/#3-install-other-dependencies","title":"(3) Install other dependencies","text":"<pre><code>pip install jupyter notebook matplotlib tensorboard scipy librosa unidecode einops cython tqdm pyyaml pesq pystoi torch-pesq torchmetrics\n</code></pre>"},{"location":"train/","title":"Training","text":"<p>Before you start training, make sure to prepare datasets.  </p>"},{"location":"train/#training-code","title":"Training code","text":"<p>Codes for training is in <code>wrappers/ns.py</code> (for Voicebank-Demand) and <code>wrappers/ns_on_the_fly.py</code> (for DNS-Challenge). Training codes perform the following jobs. 1. For every epoch, they train a model and writes losses to the tensorboard. If <code>train.plot_params_and_grad</code> is set to true, they write parameters and gradients to the tensorboard. 2. For every epoch, they validates and writes losses to the tensorboard. 3. For every <code>train.save_interval</code> epochs, it saves the checkpoints. 4. For every <code>infer.interval</code> epochs, it inferences some samples and writes audios to the tensorboard. 5. For every <code>pesq.interval</code> epochs, it calculates objective metrics (PESQ and STOI) and writes to the   tensorboard.  </p> <p>You can set those <code>interval</code>s in <code>config/*.yaml</code> files.</p>"},{"location":"train/#training-recipes","title":"Training recipes","text":"<ul> <li>FastEnhancer</li> <li>BSRNN</li> <li>FSPEN</li> <li>LiSenNet</li> </ul>"},{"location":"train/#cleaning-checkpoints","title":"Cleaning checkpoints","text":"<p>After training, many checkpoints are generated. In most cases, we only need the last one. If you want to remove all the checkpoints except the last one, This section is for you. Suppose the log directory looks like this:</p> <pre><code>logs\n\u251c\u2500 vbd\n|  \u251c\u2500 fastenhancer_b\n|  |  \u251c\u2500 00020.pth\n|  |  \u251c\u2500 ...\n|  |  \u251c\u2500 00480.pth\n|  |  \u2514\u2500 00500.pth\n|  \u2514\u2500 fastenhancer_t\n|     \u251c\u2500 00020.pth\n|     \u251c\u2500 ...\n|     \u251c\u2500 00480.pth\n|     \u2514\u2500 00500.pth\n\u2514\u2500 dns\n   \u2514\u2500 fastenhancer_b\n      \u251c\u2500 00020.pth\n      \u251c\u2500 ...\n      \u251c\u2500 00480.pth\n      \u2514\u2500 00500.pth</code></pre> <p>If you want to delete all checkpoints except the last one in <code>logs/vbd</code>, run the following code:</p> <pre><code>python scripts/clean_checkpoints.py -n vbd --delete</code></pre> <p>If you just want to check how many checkpoints you can delete, instead of actually deleting them, run without the <code>--delete</code> flag:</p> <pre><code>python scripts/clean_checkpoints.py -n vbd</code></pre> <p>After deleting the checkpoints in <code>logs/vbd</code>, the log directory will be:</p> <pre><code>logs\n\u251c\u2500 vbd\n|  \u251c\u2500 fastenhancer_b\n|  |  \u2514\u2500 00500.pth\n|  \u2514\u2500 fastenhancer_t\n|     \u2514\u2500 00500.pth\n\u2514\u2500 dns\n   \u2514\u2500 fastenhancer_b\n      \u251c\u2500 00020.pth\n      \u251c\u2500 ...\n      \u251c\u2500 00480.pth\n      \u2514\u2500 00500.pth</code></pre>"},{"location":"train/#experience-sharing","title":"Experience sharing","text":"<p>Except for Voicebank-Demand at 16kHz sampling rate, we recommend not to use PESQLoss. The reasons are: 1. It harms stable training. 2. It doesn't improve other metrics so much (in VoiceBank-Demand @ 16kHz, other metrics marginally improves, so we included it in our paper). 3. The loss includes multiple IIR filter calculations, resulting in increased training time.  </p> <p>In our experiments, we found that using MetricGAN instead of PESQLoss shows inferior results. MetricGAN achieved a smaller PESQ improvement than PESQLoss and degraded other objective metrics. However, these results may vary depending on the loss functions, batch size, datasets, and models.</p>"},{"location":"train/bsrnn/","title":"BSRNN","text":"<ul> <li>Original paper that proposed BSRNN: Paper [1]  </li> <li>BSRNN applied to noise suppression: Paper [2]  </li> <li>BSRNN model size scaling: Paper [3] | Github </li> </ul> <p>We implemented a streaming BSRNN with batch normalization. We followed [3] for the configurations of different sizes.  </p> <p>[1]: Y. Luo and J. Yu, \u201cMusic source separation with band-split RNN\u201d, IEEE/ACM Trans. ASLP, vol. 31, pp. 1893-1901, 2023. [2]: J. Yu, H. Chen, Y. Luo, R. Gu, and C. Weng, \u201cHigh fidelity speech enhancement with band-split RNN,\u201d in Proc. Interspeech, 2023, pp. 2483\u20132487. [3]: W. Zhang, K. Saijo, J.-w. Jung, C. Li, S. Watanabe, and Y. Qian, \u201cBeyond performance plateaus: A comprehensive study on scalability in speech enhancement,\u201d in Proc. Interspeech, 2024, pp. 1740-1744.  </p>"},{"location":"train/bsrnn/#training","title":"Training","text":"<ul> <li>Model: BSRNN-xxt</li> <li>Dataset: Voicebank-Demand at 16kHz sampling rate. </li> <li>Number of GPUs: 1</li> <li>Batch size: 64</li> <li>Mixed-precision training with fp16: False</li> <li>Path to save config, tensorboard logs, and checkpoints: <code>logs/vbd/16khz/bsrnn_xxt</code></li> </ul> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n vbd/16khz/bsrnn_xxt \\\n  -c configs/others/bsrnn_xxt.yaml \\\n  -p train.batch_size=64 valid.batch_size=64 \\\n  -f</code></pre> <p>or</p> <pre><code>CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\\n  train_torchrun.py \\\n  -n vbd/16khz/bsrnn_xxt \\\n  -c configs/others/bsrnn_xxt.yaml \\\n  -p train.batch_size=64 valid.batch_size=64 \\\n  -f</code></pre> <p>Options: - <code>-n</code> (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - <code>-c</code> (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - <code>-p</code> (Optional): Parameters after this will update the configuration. - <code>-f</code> (Optional): If the base directory already exists and <code>-c</code> flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file.  </p>"},{"location":"train/bsrnn/#resume-training","title":"Resume Training","text":"<p>Suppose you stopped the training. To load the saved checkpoint at <code>logs/vbd/16khz/bsrnn_xxt</code> and resume the training, use the code below:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n vbd/16khz/bsrnn_xxt</code></pre> <p>or</p> <pre><code>CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\\n  train_torchrun.py \\\n  -n vbd/16khz/bsrnn_xxt</code></pre>"},{"location":"train/bsrnn/#test-the-training-code","title":"Test the training code","text":"<p>Before you start training, we recommend to run the following test code:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n delete_it \\\n  -c configs/others/bsrnn_xxt.yaml \\\n  -p train.test=True pesq.interval=1 \\\n  -f</code></pre> <p>By setting <code>train.test=True</code>, it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting <code>pesq.interval=1</code>, it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete <code>logs/delete_it</code> directory after the test.</p>"},{"location":"train/bsrnn/#about-optimizer_groups","title":"About optimizer_groups","text":"<p>In <code>configs/others/bsrnn_xxt.yaml</code>, you may notice that instead of using <code>train.optimizer=AdamP</code> as in FastEnhancer, it uses <code>train.optimizer=AdamW</code>. This is because there's no scale-invariant parameter in BSRNN. It employs pre-activation and no weight norm is applied. In such case, AdamP is same as `AdamW.  </p>"},{"location":"train/fastenhancer/","title":"FastEnhancer","text":"<p>Paper [1] | Github </p> <p>[1] S. Ahn, J. Han, B. J. Woo, and N. S. Kim, \u201cFastEnhancer: Speed-optimized streaming neural speech enhancement,\u201d, arXiv:2509.21867, 2025.  </p>"},{"location":"train/fastenhancer/#training-fastenhancer-large-on-voicebank-demand-16khz","title":"Training FastEnhancer-Large on Voicebank-Demand 16kHz","text":"<ul> <li>Model: FastEnhancer-Large</li> <li>Dataset: Voicebank-Demand at 16kHz sampling rate</li> <li>Number of GPUs: 4</li> <li>Batch size: 16/GPU, total 64</li> <li>Mixed-precision training with fp16: True</li> <li>Path to save config, tensorboard logs, and checkpoints: <code>logs/vbd/16khz/fastenhancer_l</code></li> </ul> <pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\\n  -n vbd/16khz/fastenhancer_l \\\n  -c configs/fastenhancer/l.yaml \\\n  -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\\n  -f</code></pre> <p>or</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nproc_per_node=4 \\\n  train_torchrun.py \\\n  -n vbd/16khz/fastenhancer_l \\\n  -c configs/fastenhancer/l.yaml \\\n  -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\\n  -f</code></pre>"},{"location":"train/fastenhancer/#training-fastenhancer-huge-on-dns-challenge-16khz","title":"Training FastEnhancer-Huge on DNS-Challenge 16kHz","text":"<ul> <li>Model: FastEnhancer-Huge-Noncausal</li> <li>Dataset: DNS-Challenge at 16kHz sampling rate</li> <li>Number of GPUs: 4</li> <li>Batch size: 16/GPU, total 64</li> <li>Mixed-precision training with fp16: True</li> <li>Path to save config, tensorboard logs, and checkpoints: <code>logs/dns/16khz/fastenhancer_h</code></li> </ul> <pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\\n  -n dns/16khz/fastenhancer_h \\\n  -c configs/fastenhancer/huge_noncausal_dns.yaml \\\n  -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\\n  -f</code></pre> <p>Options: - <code>-n</code> (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - <code>-c</code> (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - <code>-p</code> (Optional): Parameters after this will update the configuration. - <code>-f</code> (Optional): If the base directory already exists and <code>-c</code> flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file.  </p>"},{"location":"train/fastenhancer/#resume-training","title":"Resume Training","text":"<p>Suppose you stopped the training. To load the saved checkpoint at <code>logs/vbd/16khz/fastenhancer_l</code> and resume the training, use the code below:</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\\n  -n vbd/16khz/fastenhancer_l</code></pre> <p>or</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nproc_per_node=4 \\\n  train_torchrun.py \\\n  -n vbd/16khz/fastenhancer_l</code></pre>"},{"location":"train/fastenhancer/#test-the-training-code","title":"Test the training code","text":"<p>Before you start training, we recommend to run the following test code:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n delete_it \\\n  -c configs/fastenhancer/l.yaml \\\n  -p train.test=True pesq.interval=1 \\\n  -f</code></pre> <p>By setting <code>train.test=True</code>, it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting <code>pesq.interval=1</code>, it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete <code>logs/delete_it</code> directory after the test.</p>"},{"location":"train/fastenhancer/#about-optimizer_groups","title":"About optimizer_groups","text":"<p>In <code>configs/fastenhancer/l.yaml</code>, you can see a large code block for <code>train.optimizer_groups</code>. This section is for ones who want to know the details of those lines.  </p> <p>The lines below set <code>weight_decay=0</code> and <code>projection=disabled</code> for <code>weight_g</code>s of GRUs and <code>scale</code> parameter of the final convolution of the decoder. If we apply weight_norm to GRUs, we empirically found that setting <code>weight_decay=0</code> to <code>weight_g</code>s improves performance. We believe this is because of <code>tanh</code> and <code>sigmoid</code> functions applied after weight-input multiplication. Also, intuitively, we should not apply <code>weight_decay</code> to the final mask prediction.</p> <pre><code>-\n    regex_list:\n        - \"rf_block\\\\.\\\\d\\\\.rnn\\\\.parametrizations.+original0$\" # GRU weight_g\n        - \"dec_post\\\\.3\\\\.scale\"        # scale parameter of the final conv\n    weight_decay: 0\n    projection: disabled</code></pre> <p>The lines below set <code>projection=channelwise</code> and <code>projection=layerwise</code> to appropriate parameters. Note that originally <code>AdamP(projection=auto)</code> can automatically handle them by detecting scale-invariant parameters. However, we found that when using mixed-precision training (by setting <code>train.fp16=True</code>), <code>AdamP</code> often failed to detect scale-invariance because of the numerical error. Therefore, we manually set <code>projection</code>s.</p> <pre><code>-\n    regex_list:\n        - \".+parametrizations.+original1$\"  # weight_v\n        - \"enc_pre\\\\.0\\\\.weight\"            # conv1d before BN\n    projection: channelwise\n-\n   regex_list:\n        - \"dec_post\\\\.3\\\\.weight\"           # final conv\n    projection: layerwise</code></pre>"},{"location":"train/fspen/","title":"FSPEN","text":"<p>Paper[1] Since there's no official implementation, we faithfully re-implemented the model architecture following the paper and configured the training settings identically to FastEnhancer for fair comparison.</p> <p>[1]: L. Yang, W. Liu, R. Meng, G. Lee, S. Baek, and H.-G. Moon, \u201cFspen: an ultra-lightweight network for real time speech enahncment,\u201d in Proc. IEEE ICASSP, 2024, pp. 10671\u201310675.  </p>"},{"location":"train/fspen/#training","title":"Training","text":"<ul> <li>Model: FSPEN</li> <li>Dataset: Voicebank-Demand at 16kHz sampling rate. </li> <li>Number of GPUs: 1</li> <li>Batch size: 64</li> <li>Mixed-precision training with fp16: False</li> <li>Path to save config, tensorboard logs, and checkpoints: <code>logs/vbd/16khz/fspen</code></li> </ul> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n vbd/16khz/fspen \\\n  -c configs/others/fspen.yaml \\\n  -p train.batch_size=64 valid.batch_size=64 \\\n  -f</code></pre> <p>or</p> <pre><code>CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\\n  train_torchrun.py \\\n  -n vbd/16khz/fspen \\\n  -c configs/others/fspen.yaml \\\n  -p train.batch_size=64 valid.batch_size=64 \\\n  -f</code></pre> <p>Options: - <code>-n</code> (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - <code>-c</code> (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - <code>-p</code> (Optional): Parameters after this will update the configuration. - <code>-f</code> (Optional): If the base directory already exists and <code>-c</code> flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file.  </p>"},{"location":"train/fspen/#resume-training","title":"Resume Training","text":"<p>Suppose you stopped the training. To load the saved checkpoint at <code>logs/vbd/16khz/fspen</code> and resume the training, use the code below:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n vbd/16khz/fspen</code></pre> <p>or</p> <pre><code>CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\\n  train_torchrun.py \\\n  -n vbd/16khz/fspen</code></pre>"},{"location":"train/fspen/#test-the-training-code","title":"Test the training code","text":"<p>Before you start training, we recommend to run the following test code:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n delete_it \\\n  -c configs/others/fspen.yaml \\\n  -p train.test=True pesq.interval=1 \\\n  -f</code></pre> <p>By setting <code>train.test=True</code>, it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting <code>pesq.interval=1</code>, it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete <code>logs/delete_it</code> directory after the test.</p>"},{"location":"train/fspen/#about-optimizer_groups","title":"About optimizer_groups","text":"<p>In <code>configs/fastenhancer/l.yaml</code>, take a look at <code>train.optimizer_groups</code>. There are only two scale-invariant parameters in FSPEN, and we manually set them.</p>"},{"location":"train/lisennet/","title":"LiSenNet","text":"<p>Paper [1] | Github The official implementation includes input normalization and Griffin-Lim, so it is not streamable. To make the model streamable, input normalization is removed, and instead of Griffin-Lim, the model predicts a complex mask. We configured the training settings identically to FastEnhancer for fair comparison.</p> <p>[1]: H. Yan, J. Zhang, C. Fan, Y. Zhou, and P. Liu, \u201cLiSenNet: Lightweight sub-band and dual-path modeling for real-time speech enhancement,\u201d in Proc. IEEE ICASSP, 2025, pp. 1\u20135.  </p>"},{"location":"train/lisennet/#training","title":"Training","text":"<ul> <li>Model: LiSenNet</li> <li>Dataset: Voicebank-Demand at 16kHz sampling rate. </li> <li>Number of GPUs: 1</li> <li>Batch size: 64</li> <li>Mixed-precision training with fp16: False</li> <li>Path to save config, tensorboard logs, and checkpoints: <code>logs/vbd/16khz/lisennet</code></li> </ul> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n vbd/16khz/lisennet \\\n  -c configs/others/lisennet.yaml \\\n  -p train.batch_size=64 valid.batch_size=64 \\\n  -f</code></pre> <p>or</p> <pre><code>CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\\n  train_torchrun.py \\\n  -n vbd/16khz/lisennet \\\n  -c configs/others/lisennet.yaml \\\n  -p train.batch_size=64 valid.batch_size=64 \\\n  -f</code></pre> <p>Options: - <code>-n</code> (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - <code>-c</code> (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - <code>-p</code> (Optional): Parameters after this will update the configuration. - <code>-f</code> (Optional): If the base directory already exists and <code>-c</code> flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file.  </p>"},{"location":"train/lisennet/#resume-training","title":"Resume Training","text":"<p>Suppose you stopped the training. To load the saved checkpoint at <code>logs/vbd/16khz/lisennet</code> and resume the training, use the code below:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n vbd/16khz/lisennet</code></pre> <p>or</p> <pre><code>CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\\n  train_torchrun.py \\\n  -n vbd/16khz/lisennet</code></pre>"},{"location":"train/lisennet/#test-the-training-code","title":"Test the training code","text":"<p>Before you start training, we recommend to run the following test code:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py \\\n  -n delete_it \\\n  -c configs/others/lisennet.yaml \\\n  -p train.test=True pesq.interval=1 \\\n  -f</code></pre> <p>By setting <code>train.test=True</code>, it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting <code>pesq.interval=1</code>, it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete <code>logs/delete_it</code> directory after the test.</p>"},{"location":"train/lisennet/#about-optimizer","title":"About Optimizer","text":"<p>In <code>configs/others/lisennet.yaml</code>, you may notice that instead of using <code>train.optimizer=AdamP</code> as in FastEnhancer, it uses <code>train.optimizer=AdamW</code>. This is because there's no scale-invariant parameter in LiSenNet. It employs LayerNorm and no weight norm is applied. In such case, AdamP is same as AdamW.  </p> <p>For PReLU weights, one should not apply weight decay, so we set <code>train.optimizer_groups</code> appropriately.</p>"}]}